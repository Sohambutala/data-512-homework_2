{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing US Population and Wikipedia Articles for Cities\n",
    "\n",
    "## Introduction \n",
    "\n",
    "In this Jupyter notebook we perform a series of API calls to gather data from the Wikipedia endpoints. We also use the population data and list of US cities by state present in the data folder. After merging all three datasets we are set to analyze the correlation between US population and Wikipedia Articles. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Necessary Libraries\n",
    "\n",
    "#### We begin by importing crucial Python libraries to facilitate our analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import json, time\n",
    "import requests\n",
    "import concurrent\n",
    "from threading import Lock\n",
    "\n",
    "# Initialize a lock to synchronize access to shared resources\n",
    "lock = Lock() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading and Preprocessing Data\n",
    "\n",
    "#### In this section, we focus on loading CSV files containing data about US cities, populations, and regions. We preprocess the data to remove irrelevant columns, handle missing values, and prepare it for further analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading CSV files into DataFrames for city, population, and region data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV files into DataFrames for city, population, and region data\n",
    "cities_csv_file_path = 'data/us_cities_by_state_SEPT.2023.csv'\n",
    "city_data = pd.read_csv(cities_csv_file_path)\n",
    "\n",
    "pop_csv_file_path = 'data/NST-EST2022-ALLDATA.csv'\n",
    "pop_data = pd.read_csv(pop_csv_file_path)\n",
    "\n",
    "reg_csv_file_path = 'data/US States by Region - US Census Bureau.xlsx'\n",
    "reg_data = pd.read_excel(reg_csv_file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing region data by filling missing values and dropping rows with NaN in 'STATE' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REGION</th>\n",
       "      <th>DIVISION</th>\n",
       "      <th>STATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Northeast</td>\n",
       "      <td>New England</td>\n",
       "      <td>Connecticut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Northeast</td>\n",
       "      <td>New England</td>\n",
       "      <td>Maine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Northeast</td>\n",
       "      <td>New England</td>\n",
       "      <td>Massachusetts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Northeast</td>\n",
       "      <td>New England</td>\n",
       "      <td>New Hampshire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Northeast</td>\n",
       "      <td>New England</td>\n",
       "      <td>Rhode Island</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      REGION     DIVISION          STATE\n",
       "2  Northeast  New England    Connecticut\n",
       "3  Northeast  New England          Maine\n",
       "4  Northeast  New England  Massachusetts\n",
       "5  Northeast  New England  New Hampshire\n",
       "6  Northeast  New England   Rhode Island"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess region data: fill missing values and drop rows with NaN in 'STATE' column\n",
    "reg_data['REGION'] = reg_data['REGION'].fillna(method='ffill')\n",
    "reg_data['DIVISION'] = reg_data['DIVISION'].fillna(method='ffill')\n",
    "reg_data.dropna(subset=['STATE'], inplace=True)\n",
    "reg_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing population data by removing unnecessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME</th>\n",
       "      <th>POPESTIMATE2022</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>United States</td>\n",
       "      <td>333287557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Northeast Region</td>\n",
       "      <td>57040406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New England</td>\n",
       "      <td>15129548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Middle Atlantic</td>\n",
       "      <td>41910858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Midwest Region</td>\n",
       "      <td>68787595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               NAME  POPESTIMATE2022\n",
       "0     United States        333287557\n",
       "1  Northeast Region         57040406\n",
       "2       New England         15129548\n",
       "3   Middle Atlantic         41910858\n",
       "4    Midwest Region         68787595"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove unnecessary columns from population data\n",
    "columns_to_remove = ['NPOPCHG_2020', 'NPOPCHG_2021', 'NPOPCHG_2022',\n",
    "                     'BIRTHS2020', 'BIRTHS2021', 'BIRTHS2022',\n",
    "                     'DEATHS2020', 'DEATHS2021', 'DEATHS2022',\n",
    "                     'NATURALCHG2020', 'NATURALCHG2021', 'NATURALCHG2022',\n",
    "                     'INTERNATIONALMIG2020', 'INTERNATIONALMIG2021', 'INTERNATIONALMIG2022',\n",
    "                     'DOMESTICMIG2020', 'DOMESTICMIG2021', 'DOMESTICMIG2022',\n",
    "                     'NETMIG2020', 'NETMIG2021', 'NETMIG2022',\n",
    "                     'RESIDUAL2020', 'RESIDUAL2021', 'RESIDUAL2022',\n",
    "                     'RBIRTH2021', 'RBIRTH2022',\n",
    "                     'RDEATH2021', 'RDEATH2022',\n",
    "                     'RNATURALCHG2021', 'RNATURALCHG2022',\n",
    "                     'RINTERNATIONALMIG2021', 'RINTERNATIONALMIG2022',\n",
    "                     'RDOMESTICMIG2021', 'RDOMESTICMIG2022',\n",
    "                     'RNETMIG2021', 'RNETMIG2022','SUMLEV','REGION','DIVISION','STATE',\n",
    "                     'ESTIMATESBASE2020','POPESTIMATE2020','POPESTIMATE2021']\n",
    "\n",
    "pop_data.drop(columns=columns_to_remove, inplace=True)\n",
    "pop_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying entries in the Population table not present in the Regional CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entries in the Population table not in the Regional CSV:\n",
      "                    NAME  POPESTIMATE2022\n",
      "0          United States        333287557\n",
      "1       Northeast Region         57040406\n",
      "2            New England         15129548\n",
      "3        Middle Atlantic         41910858\n",
      "4         Midwest Region         68787595\n",
      "5     East North Central         47097779\n",
      "6     West North Central         21689816\n",
      "7           South Region        128716192\n",
      "8         South Atlantic         67452940\n",
      "9     East South Central         19578002\n",
      "10    West South Central         41685250\n",
      "11           West Region         78743364\n",
      "12              Mountain         25514320\n",
      "13               Pacific         53229044\n",
      "22  District of Columbia           671803\n",
      "65           Puerto Rico          3221789\n"
     ]
    }
   ],
   "source": [
    "# Identify entries in the Population table not present in the Regional CSV\n",
    "entries_not_in_first = pop_data[~pop_data['NAME'].isin(reg_data['STATE'])]\n",
    "\n",
    "print(\"\\nEntries in the Population table not in the Regional CSV:\")\n",
    "print(entries_not_in_first)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Merging Population and Region Data\n",
    "#### Next, we merge population data with region information based on common attributes like state names. This enables a comprehensive view of how the population is distributed across various regions of the US."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging population and region data based on 'NAME' and 'STATE'. Rename columns for consistency and drop columns which are not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Population</th>\n",
       "      <th>STATE</th>\n",
       "      <th>REGION_DIVISION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5074296</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>South_East South Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>733583</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>West_Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7359197</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>West_Mountain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3045637</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>South_West South Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39029342</td>\n",
       "      <td>California</td>\n",
       "      <td>West_Pacific</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Population       STATE           REGION_DIVISION\n",
       "0     5074296     Alabama  South_East South Central\n",
       "1      733583      Alaska              West_Pacific\n",
       "2     7359197     Arizona             West_Mountain\n",
       "3     3045637    Arkansas  South_West South Central\n",
       "4    39029342  California              West_Pacific"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge population and region data based on 'NAME' and 'STATE'\n",
    "data = pd.merge(pop_data, reg_data, left_on='NAME', right_on='STATE', how='inner')\n",
    "data['REGION_DIVISION'] = data['REGION'] + '_' + data['DIVISION']\n",
    "data.rename(columns={'POPESTIMATE2022': 'Population'}, inplace=True)\n",
    "\n",
    "data.drop(columns=['NAME', 'REGION', 'DIVISION'], inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing city data by dropping duplicates, removing unnecessary columns, and adding columns for revision_id and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>page_title</th>\n",
       "      <th>revision_id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Abbeville, Alabama</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Adamsville, Alabama</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Addison, Alabama</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Akron, Alabama</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Alabaster, Alabama</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     state           page_title revision_id prediction\n",
       "0  Alabama   Abbeville, Alabama        None       None\n",
       "1  Alabama  Adamsville, Alabama        None       None\n",
       "2  Alabama     Addison, Alabama        None       None\n",
       "3  Alabama       Akron, Alabama        None       None\n",
       "4  Alabama   Alabaster, Alabama        None       None"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess city data: drop duplicates, remove unnecessary columns, and add columns for revision_id and prediction\n",
    "city_data.drop_duplicates(subset='page_title')\n",
    "city_data.drop(columns=['url'], inplace=True)\n",
    "city_data = city_data.assign(revision_id=None, prediction=None)\n",
    "city_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gathering Wikipedia Articles for Cities\n",
    "#### This part of the notebook involves a deep dive into Wikipedia articles associated with cities. We fetch revision IDs and predict article quality using the ORES (Objective Revision Evaluation Service) API. To expedite this process for a large dataset, we employ multithreading."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining necessary API endpoints and request headers for Wikipedia API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define necessary API endpoints and request headers for Wikipedia API\n",
    "API_ENWIKIPEDIA_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "EMAIL = \"sbutala@uw.edu\"\n",
    "\n",
    "INFO_REQUEST_HEADERS = {\n",
    "    'User-Agent': EMAIL+', University of Washington, MSDS DATA 512 - AUTUMN 2023',\n",
    "}\n",
    "\n",
    "ACCESS_TOKEN = \"eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJhdWQiOiIwZjM3MWM0Yzc4MzExZjc0OGQwOTg0ZWU5ODZkYjEwZSIsImp0aSI6ImRlYTY2ZWU1MTRiMWJlMjI0YzQ3NDU2NDVhNmQ1M2NjNDI0ZWRhODE3YWZkZDhmNDA2MWJjZWIxZDAxOWE2ZjFmYjgwMjA5YjhlMDE2ODI1IiwiaWF0IjoxNjk3NTA3ODg2LjM3NjE2MiwibmJmIjoxNjk3NTA3ODg2LjM3NjE2NSwiZXhwIjozMzI1NDQxNjY4Ni4zNzM5NzQsInN1YiI6Ijc0MDIwMzI3IiwiaXNzIjoiaHR0cHM6Ly9tZXRhLndpa2ltZWRpYS5vcmciLCJyYXRlbGltaXQiOnsicmVxdWVzdHNfcGVyX3VuaXQiOjUwMDAsInVuaXQiOiJIT1VSIn0sInNjb3BlcyI6WyJiYXNpYyJdfQ.EyvW7hk3u4AAL-n6_CdgxLmv5rvexcgndr6RrT7KOSMI1JqJeAvwTZqBio5U2HmDTWjiCOfd51S1p5OKC_yQA3QOvYRIenfxx9daxRtdvIrYnwxtrl59H4qQmgfJAdraSMpk5J1IenQj01MsrI8D2mV_es67Q2cAPxAjT41Ul1WmxOrRHK8t7SFJZGLNk2pTluwA54p9XLEg4fKuu1uI6UHD4omW6CCpgBVqXf37sVvSYCXQe-4GDEyGdx_VumgX3hOzEm-ZCa3k1AEkKjfFPLpzLTSC7Fz9vCRenvNtGb05E4NWgRHkFjPvHQzNlmTNKoIolCTFD_ij-Tx7I6H4De15Y8-HIzB9kwJfmueGjkQOWpod6iddPi0w6ChCdJ49Reg5olnLHSUWOzYwAfgqyFh7UBmXM20gd_Wkh79m32I2_7W3wJw_SqGUzDaYMM1EOFZl4EWnMWhWgyzAJcVmi01Wrd638CZxgfEfgIKdciPBkpP_GM-Kas3iKHVmrXlep76gWZZy72wrffu9AyJNzz4dX8ONhSWuNSD2gMEO5PlJy5RdCbHTM0jPX8Z73w76V2VBj1P89EboAnd26D4mb8qxgAqiDljE7__0vleFlFAb80HSIjpuHOtusvzT4pofeBj14hPCxgq22euWsOuhC1ZIzf7W6TmEOigqw6UGm3I\"\n",
    "\n",
    "ORES_REQUEST_HEADERS = {\n",
    "    'User-Agent': EMAIL+\", University of Washington, MSDS DATA 512 - AUTUMN 2023\",\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': \"Bearer \"+ACCESS_TOKEN\n",
    "}\n",
    "\n",
    "API_ORES_LIFTWING_ENDPOINT = \"https://api.wikimedia.org/service/lw/inference/v1/models/enwiki-articlequality:predict\"\n",
    "\n",
    "ORES_REQUEST_DATA = {\n",
    "    \"lang\":        \"en\",     \n",
    "    \"features\":    True\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining functions to request ORES score and page info for an article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to request ORES score for an article\n",
    "def request_ores_score_per_article(article_revid = None):\n",
    "    \n",
    "    headers = ORES_REQUEST_HEADERS\n",
    "\n",
    "    request_data = ORES_REQUEST_DATA\n",
    "\n",
    "    if article_revid:\n",
    "        request_data['rev_id'] = article_revid\n",
    "    else:\n",
    "        raise Exception(\"Must provide an article revision id (rev_id) to score articles\")\n",
    "    \n",
    "    request_url = API_ORES_LIFTWING_ENDPOINT\n",
    "\n",
    "    try:\n",
    "        response = requests.post(request_url, headers=headers, data=json.dumps(request_data))\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n",
    "\n",
    "# Define a function to request page info for an article\n",
    "def request_pageinfo_per_article(article_title = None):\n",
    "    \n",
    "    if article_title:\n",
    "        request_template = {\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"titles\": article_title,\n",
    "            \"prop\": \"info\"\n",
    "        }\n",
    "    else:\n",
    "        raise Exception(\"Must supply an article title to make a pageinfo request.\")\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        response = requests.get(API_ENWIKIPEDIA_ENDPOINT, headers=INFO_REQUEST_HEADERS, params=request_template)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function to process each city and request ORES scores and revision_ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to process each city and request ORES score and revision_id\n",
    "def process_city(index, city):\n",
    "    try:\n",
    "        resp = request_pageinfo_per_article(city.page_title)\n",
    "        first_key = next(iter(resp['query']['pages']))\n",
    "        lastrevid = resp['query']['pages'][first_key]['lastrevid']\n",
    "\n",
    "        score = request_ores_score_per_article(article_revid=lastrevid)\n",
    "        pred = score['enwiki']['scores'][str(lastrevid)]['articlequality']['score']['prediction']\n",
    "        with lock:\n",
    "            city_data.loc[index, 'revision_id'] = str(lastrevid)\n",
    "            city_data.loc[index, 'prediction'] = pred\n",
    "    except Exception as e:\n",
    "        print(score)\n",
    "        print(e)\n",
    "        with lock:\n",
    "            city_data.loc[index, 'revision_id'] = None\n",
    "            city_data.loc[index, 'prediction'] = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting concurrent processing of city data to request ORES scores and revision_ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start concurrent processing of city data to request ORES scores and revision_ids\n",
    "start_index = 0\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = []\n",
    "    for index, city in city_data.iterrows():\n",
    "        if city.revision_id == None or city.prediction == None:\n",
    "            futures.append(executor.submit(process_city, index, city))\n",
    "            start_index += 1\n",
    "            if start_index%5 == 0:\n",
    "                time.sleep(1)\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        future.result()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if prediction results exist for all the records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction results exists for all the records, in case it doesn't feel free to execute the previous cell again or even in case of any network failure\n",
    "city_data['prediction'].isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Merge and Create Dataset\n",
    "#### In this step we merge both the datasets obtained from step 3 and 4 and merge together to output as a csv file names **wp_scored_city_articles_by_state**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin merging, we clean state names by handling special cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California',\n",
       "       'Colorado', 'Delaware', 'Florida', 'Georgia', 'Hawaii', 'Idaho',\n",
       "       'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana',\n",
       "       'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota',\n",
       "       'Mississippi', 'Missouri', 'Montana', 'Nevada', 'New Hampshire',\n",
       "       'New Jersey', 'New Mexico', 'New York', 'North Carolina',\n",
       "       'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania',\n",
       "       'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee',\n",
       "       'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington',\n",
       "       'West Virginia', 'Wisconsin', 'Wyoming'], dtype=object)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_state_name(state):\n",
    "    # Handle 'Georgia_(U.S._state)' case\n",
    "    if state.startswith('Georgia'):\n",
    "        return 'Georgia'\n",
    "    else:\n",
    "        return state\n",
    "    \n",
    "city_data['state'] = city_data['state'].str.replace('_', ' ')\n",
    "city_data['state'] = city_data['state'].apply(clean_state_name)\n",
    "city_data['state'].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a list of states which are missing in the Wikipedia's list of US cities by state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing States from the Wiki list\n",
      "{'Connecticut', 'Nebraska'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing States from the Wiki list\")\n",
    "print(set(data['STATE'].unique()) - set(city_data['state'].unique()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we join both the datasets with inner join on the state column. Next we drop one of the state columns to reduce redundancy and then rename columns for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>population</th>\n",
       "      <th>state</th>\n",
       "      <th>regional_division</th>\n",
       "      <th>state</th>\n",
       "      <th>article_title</th>\n",
       "      <th>revision_id</th>\n",
       "      <th>article_quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5074296</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>South_East South Central</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Abbeville, Alabama</td>\n",
       "      <td>1171163550</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5074296</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>South_East South Central</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Adamsville, Alabama</td>\n",
       "      <td>1177621427</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5074296</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>South_East South Central</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Addison, Alabama</td>\n",
       "      <td>1168359898</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5074296</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>South_East South Central</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Akron, Alabama</td>\n",
       "      <td>1165909508</td>\n",
       "      <td>GA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5074296</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>South_East South Central</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Alabaster, Alabama</td>\n",
       "      <td>1179139816</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   population    state         regional_division    state  \\\n",
       "0     5074296  Alabama  South_East South Central  Alabama   \n",
       "1     5074296  Alabama  South_East South Central  Alabama   \n",
       "2     5074296  Alabama  South_East South Central  Alabama   \n",
       "3     5074296  Alabama  South_East South Central  Alabama   \n",
       "4     5074296  Alabama  South_East South Central  Alabama   \n",
       "\n",
       "         article_title revision_id article_quality  \n",
       "0   Abbeville, Alabama  1171163550               C  \n",
       "1  Adamsville, Alabama  1177621427               C  \n",
       "2     Addison, Alabama  1168359898               C  \n",
       "3       Akron, Alabama  1165909508              GA  \n",
       "4   Alabaster, Alabama  1179139816               C  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_data = pd.merge(data, city_data, left_on='STATE', right_on='state', how='inner')\n",
    "out_data.drop(columns=['state'])\n",
    "out_data.rename(columns={'page_title': 'article_title', 'prediction':'article_quality',\n",
    "                     'STATE':'state','REGION_DIVISION':'regional_division', 'Population':'population'\n",
    "                     }, inplace=True)\n",
    "out_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we output the data to a file and also store the output from the Wikipedia API (step 4) in a separate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/wp_scored_city_articles_by_state.csv'\n",
    "\n",
    "# Save the updated DataFrame to CSV\n",
    "out_data.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/city_prediction.csv'\n",
    "\n",
    "# Save the updated DataFrame to CSV\n",
    "city_data.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "#### With the above steps we now have a cleaned dataset to work on and analyze if there is any relation between the population and Wikipedia articles."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
